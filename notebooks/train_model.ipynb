{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import glob\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(email_msg):\n",
    "    \"\"\"Grab the body from a parsed email object\"\"\"\n",
    "    body = \"\"\n",
    "    if email_msg.is_multipart():\n",
    "        for payload in email_msg.get_payload():\n",
    "            body+= payload.get_payload().strip()\n",
    "    else:\n",
    "        body = email_msg.get_payload().strip()\n",
    "    if body: \n",
    "        return body \n",
    "    else: return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built in python email parser to parse the input text\n",
    "email_parser = email.parser.Parser()\n",
    "from_list = []\n",
    "subject_list = []\n",
    "organization_list = []\n",
    "body_list = []\n",
    "\n",
    "for filepath in glob.iglob('../emails/*'):\n",
    "    with open(filepath,'r',encoding=\"ISO-8859-1\") as file:\n",
    "        email_obj = email_parser.parse(file)\n",
    "        from_list.append(email_obj.get('From', ''))\n",
    "        subject_list.append(email_obj.get('Subject',''))\n",
    "        organization_list.append(email_obj.get('Organization',''))\n",
    "        body_list.append(get_body(email_obj))\n",
    "        \n",
    "email_df = pd.DataFrame({'from':from_list,'subject':subject_list,'organization':organization_list,'body':body_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../emails/00026','r',encoding=\"ISO-8859-1\") as file:\n",
    "#     email_obj = email_parser.parse(file)\n",
    "#     pprint(email_obj.get(\"From\"))\n",
    "#     pprint(email_obj.get(\"Subject\"))\n",
    "#     pprint(email_obj.get(\"Organization\"))\n",
    "#     print()\n",
    "#     pprint(get_body(email_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a document by concating the subject with the email body\n",
    "documents = [subject + body for subject,body in zip(subject_list,body_list)]\n",
    "\n",
    "# remove common words and tokenize\n",
    "# Adding in many stop words to try and restrict the appearnce. Should use TF-IDF vectorizer instead of this naive approach\n",
    "\n",
    "stoplist = set('for a of the and to in re article subject who had out them been know how then did that than get does get some when his hers were more just that you like are with this was have would they their but from can what there would will all one about has not can any your one'.split())\n",
    "texts = [word_tokenize(document.lower())for document in documents]\n",
    "texts = [[word for word in document if word not in stoplist and word.isalnum() and len(word)>2] for document in texts]\n",
    "\n",
    "# remove words that appear only once or twice\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 2]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "# Converts documents into sparse bow representation\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried using HDP model because it does not require the knowledge of the number of topics apriori\n",
    "# Learned 150 topics which is far to many and resulted in incoherent topics\n",
    "from gensim.models import HdpModel\n",
    "hdp = HdpModel(corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Fit a LDA model to bag of word represenation of documents, adjust num_topics till coherent topics appear\n",
    "# Sweeped through a range of topic numbers, saw a few coherent topics but in general not great performance\n",
    "lda = LdaModel(corpus,num_topics=6,id2word=dictionary)\n",
    "\n",
    "# Showing the top words per topic\n",
    "# Again using the LDA model there still does not appear to be coherent topics\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching to sklearn and their Clustering API\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a document by concating the subject with the email body\n",
    "documents = [subject + body for subject,body in zip(subject_list,body_list)]\n",
    "\n",
    "# Use the TFidVecotrizer to create BoW represnation of documents\n",
    "# Increased the min frequency to remove alot of junk words\n",
    "vectorizer = TfidfVectorizer(min_df=3,stop_words='english')\n",
    "X= vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't know the number of clusters so Sweeped through a range, found that 5 or 6 appeared to be right\n",
    "# Religion/God\n",
    "# Encryption\n",
    "# Windows/IT help\n",
    "# Isreal,Armenia,Turkey : middle east\n",
    "# Hockey/Sports\n",
    "model=KMeans(n_clusters=6)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model.n_clusters):\n",
    "    print(f'Cluster {i}:')\n",
    "    for ind in order_centroids[i, :15]:\n",
    "        print(f'{terms[ind]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the kmeans model\n",
    "from joblib import dump, load\n",
    "dump(model, 'kmeans_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the vectorizer\n",
    "from joblib import dump, load\n",
    "dump(vectorizer, 'tfidf_vectorizer.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-13T23:21:42.313510Z",
     "iopub.status.busy": "2020-09-13T23:21:42.313091Z",
     "iopub.status.idle": "2020-09-13T23:21:42.318112Z",
     "shell.execute_reply": "2020-09-13T23:21:42.316432Z",
     "shell.execute_reply.started": "2020-09-13T23:21:42.313478Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-13T23:21:42.943473Z",
     "iopub.status.busy": "2020-09-13T23:21:42.943189Z",
     "iopub.status.idle": "2020-09-13T23:21:42.947542Z",
     "shell.execute_reply": "2020-09-13T23:21:42.946531Z",
     "shell.execute_reply.started": "2020-09-13T23:21:42.943448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Article about jersualem recieves a top topic of 4 which matches what we have seen from the top words asscociated with each cluster\n",
    "bdy = documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-13T23:21:43.619624Z",
     "iopub.status.busy": "2020-09-13T23:21:43.619326Z",
     "iopub.status.idle": "2020-09-13T23:21:43.632900Z",
     "shell.execute_reply": "2020-09-13T23:21:43.632042Z",
     "shell.execute_reply.started": "2020-09-13T23:21:43.619595Z"
    }
   },
   "outputs": [],
   "source": [
    "resp = requests.post('http://127.0.0.1:8000/predict',json={'email':bdy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-13T23:21:45.746890Z",
     "iopub.status.busy": "2020-09-13T23:21:45.746606Z",
     "iopub.status.idle": "2020-09-13T23:21:45.751648Z",
     "shell.execute_reply": "2020-09-13T23:21:45.750964Z",
     "shell.execute_reply.started": "2020-09-13T23:21:45.746864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_topic': 4, 'topic_distribution': [], 'email_topics': []}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}